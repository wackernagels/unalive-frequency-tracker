{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final project items\n",
    "- descibe the data and a summary of the stats\n",
    "- state the research question, and what exactly we're trying to express with the data\n",
    "- what is the method? why this visual? why is this one chosen?\n",
    "- what's next? what conclusion can be drawn? what comes next for analyzing the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium documentation (for reader reference): \n",
    "* https://selenium-python.readthedocs.io/\n",
    "* https://www.selenium.dev/documentation/\n",
    "\n",
    "proxy list (in case the current proxy ip can't be used): https://free-proxy-list.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing our libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#booting up selenium to automate the scraping of reddit\n",
    "\n",
    "#defining proxy ip\n",
    "#if we get rate limited we must change the proxy\n",
    "#more proxies can be accessed from the link above\n",
    "proxy = \"71.14.218.2\"\n",
    "\n",
    "#defining headless mode (if we want to run this operation without gui)\n",
    "def driversetup(proxy):\n",
    "    options= webdriver.FirefoxOptions()\n",
    "    options.add_argument(\"-headless\")\n",
    "    options.add_argument(f\"--proxy-server={proxy}\")\n",
    "    return webdriver.Firefox(options=options)\n",
    " \n",
    "browser= driversetup(proxy)\n",
    "\n",
    "#defining non-headless mode\n",
    "#browser= webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to open a link to a page\n",
    "def openlink(url):\n",
    "    browser.get(url) #open the url in browser (we will be using firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to get title and body text of post\n",
    "#also get info like time, link, etc which may be helpful\n",
    "\n",
    "def getpostinfo(url, day, keyword):\n",
    "    #opening the link\n",
    "    openlink(url)\n",
    "    time.sleep(5) #wait for page to load\n",
    "    \n",
    "    #get number representing the date\n",
    "    data= {'word' : keyword}\n",
    "    \n",
    "    #locate the top bar where you can tab between json, raw data, and header\n",
    "    topbar= browser.find_element(By.CSS_SELECTOR, \"ul[class='tabs-menu']\")\n",
    "    rawtab= topbar.find_element(By.CSS_SELECTOR, \"a[id='rawdata-tab']\") #locating rawdata tab\n",
    "    ActionChains(browser).click(rawtab).perform() #click on tab\n",
    "    \n",
    "    #identify the raw data tab\n",
    "    rawdata= browser.find_element(By.CSS_SELECTOR, \"div[id='rawdata-panel']\")\n",
    "    #get the data in text form\n",
    "    posts= rawdata.find_element(By.CSS_SELECTOR, \"pre[class='data']\").text\n",
    "    postdata= json.loads(posts) #convert the string (a dict) into a dict\n",
    "\n",
    "    data.update(postdata) #add data from query into our dictionary\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getunix(year, month, day):\n",
    "    day= datetime.datetime(year, month, day, 0, 0)\n",
    "    return int(time.mktime(day.timetuple()))-21600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit caps search results at 100, so instead we will separate our query by day and scrape the results\n",
    "#every timestamp in unix must be subtracted by 21600 because we're in central time\n",
    "\n",
    "#defining the day we want to start querying. \n",
    "#we will query every month from 1/1/2019 to 1/1/2025\n",
    "firstdayend= getunix(2019, 1, 31)\n",
    "firstdaystart= getunix(2018, 12, 31)\n",
    "\n",
    "#adding helper function so we can easily reset the time each time we query\n",
    "def resetfirstend():\n",
    "    return firstdayend\n",
    "\n",
    "def resetfirststart():\n",
    "    return firstdayend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before and after are the variables we use when querying urls\n",
    "before= firstdayend\n",
    "after= firstdaystart\n",
    "\n",
    "#defining the end of our sampling to be 1/1/2025\n",
    "lastdayend= datetime.datetime(2020, 1, 1, 0, 0)\n",
    "end= getunix(2025, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#quick debugging test\\n\\nbefore= 1704585600\\nafter= 1704499200\\nend= 1735689600\\n\\npostdata_unalive= [] #defining empty list for the data\\n\\nurl= f\"https://api.pullpush.io/reddit/search/submission/?q=unalive&subreddit=offmychest&sort=asc&before={before}&after={after}\"\\n\\nscrapedata= getpostinfo(url, after, \"unalive\")\\npostdata_unalive.append(scrapedata)\\nprint(postdata_unalive)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#quick debugging test\n",
    "\n",
    "before= 1704585600\n",
    "after= 1704499200\n",
    "end= 1735689600\n",
    "\n",
    "postdata_unalive= [] #defining empty list for the data\n",
    "\n",
    "url= f\"https://api.pullpush.io/reddit/search/submission/?q=unalive&subreddit=offmychest&sort=asc&before={before}&after={after}\"\n",
    "\n",
    "scrapedata= getpostinfo(url, after, \"unalive\")\n",
    "postdata_unalive.append(scrapedata)\n",
    "print(postdata_unalive)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got data for 1548892800 to 1546214400\n",
      "got data for 1551484800 to 1548892800\n",
      "got data for 1554076800 to 1551484800\n",
      "got data for 1556668800 to 1554076800\n",
      "got data for 1559260800 to 1556668800\n",
      "got data for 1561852800 to 1559260800\n",
      "got data for 1564444800 to 1561852800\n",
      "got data for 1567036800 to 1564444800\n",
      "got data for 1569628800 to 1567036800\n",
      "got data for 1572220800 to 1569628800\n",
      "got data for 1574812800 to 1572220800\n",
      "got data for 1577404800 to 1574812800\n",
      "got data for 1579996800 to 1577404800\n",
      "got data for 1582588800 to 1579996800\n",
      "got data for 1585180800 to 1582588800\n",
      "got data for 1587772800 to 1585180800\n",
      "got data for 1590364800 to 1587772800\n",
      "got data for 1592956800 to 1590364800\n",
      "got data for 1595548800 to 1592956800\n",
      "got data for 1598140800 to 1595548800\n",
      "got data for 1600732800 to 1598140800\n",
      "got data for 1603324800 to 1600732800\n",
      "got data for 1605916800 to 1603324800\n",
      "got data for 1608508800 to 1605916800\n",
      "got data for 1611100800 to 1608508800\n",
      "got data for 1613692800 to 1611100800\n",
      "got data for 1616284800 to 1613692800\n",
      "got data for 1618876800 to 1616284800\n",
      "got data for 1621468800 to 1618876800\n",
      "got data for 1624060800 to 1621468800\n",
      "got data for 1626652800 to 1624060800\n",
      "got data for 1629244800 to 1626652800\n",
      "got data for 1631836800 to 1629244800\n",
      "got data for 1634428800 to 1631836800\n",
      "got data for 1637020800 to 1634428800\n",
      "got data for 1639612800 to 1637020800\n",
      "got data for 1642204800 to 1639612800\n",
      "got data for 1644796800 to 1642204800\n",
      "got data for 1647388800 to 1644796800\n",
      "got data for 1649980800 to 1647388800\n",
      "got data for 1652572800 to 1649980800\n",
      "got data for 1655164800 to 1652572800\n",
      "got data for 1657756800 to 1655164800\n",
      "got data for 1660348800 to 1657756800\n",
      "got data for 1662940800 to 1660348800\n",
      "got data for 1665532800 to 1662940800\n",
      "got data for 1668124800 to 1665532800\n",
      "got data for 1670716800 to 1668124800\n",
      "got data for 1673308800 to 1670716800\n",
      "got data for 1675900800 to 1673308800\n",
      "got data for 1678492800 to 1675900800\n",
      "got data for 1681084800 to 1678492800\n",
      "got data for 1683676800 to 1681084800\n",
      "got data for 1686268800 to 1683676800\n",
      "got data for 1688860800 to 1686268800\n",
      "got data for 1691452800 to 1688860800\n",
      "got data for 1694044800 to 1691452800\n",
      "got data for 1696636800 to 1694044800\n",
      "got data for 1699228800 to 1696636800\n",
      "got data for 1701820800 to 1699228800\n",
      "got data for 1704412800 to 1701820800\n",
      "got data for 1707004800 to 1704412800\n",
      "got data for 1709596800 to 1707004800\n",
      "got data for 1712188800 to 1709596800\n",
      "got data for 1714780800 to 1712188800\n",
      "got data for 1717372800 to 1714780800\n",
      "got data for 1719964800 to 1717372800\n",
      "got data for 1722556800 to 1719964800\n",
      "got data for 1725148800 to 1722556800\n",
      "got data for 1727740800 to 1725148800\n",
      "got data for 1730332800 to 1727740800\n",
      "got data for 1732924800 to 1730332800\n",
      "got data for 1735516800 to 1732924800\n",
      "got data for 1735689600 to 1735516800\n"
     ]
    }
   ],
   "source": [
    "#scraping for 'unalive'\n",
    "postdata_unalive= [] #defining empty list for the data\n",
    "\n",
    "#exit and break do not do anything in jupyter notebook\n",
    "while before<=end:\n",
    "    url= f\"https://api.pullpush.io/reddit/search/submission/?q=unalive&subreddit=offmychest&sort=asc&before={before}&after={after}\"\n",
    "    scrapedata= getpostinfo(url, after, \"unalive\")\n",
    "    postdata_unalive.append(scrapedata)\n",
    "\n",
    "    print(f\"got data for {after} to {before}\")\n",
    "    \n",
    "    after=before\n",
    "    #if our usual increment will take us past our end year, query until end year\n",
    "    #if we are at the end year, exit the loop\n",
    "    if before==end:\n",
    "        before=before+2592000\n",
    "    elif (before+2592000)>end:\n",
    "        before= end\n",
    "    #otherwise query for the next 30 days\n",
    "    else:\n",
    "        before=before+2592000\n",
    "\n",
    "#reset the start parameters  \n",
    "before= firstdayend\n",
    "after= firstdaystart\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine our data into a singular list\n",
    "postdata= postdata_unalive\n",
    "\n",
    "#after collecting data, export it into csv format\n",
    "header = [\"word\", \"date\", \"data\", \"metadata\", \"error\"]\n",
    "with open('unalive_data.csv', 'w', encoding=\"utf-8\", newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(postdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
