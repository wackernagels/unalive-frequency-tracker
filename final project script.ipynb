{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final project items\n",
    "- descibe the data and a summary of the stats\n",
    "- state the research question, and what exactly we're trying to express with the data\n",
    "- what is the method? why this visual? why is this one chosen?\n",
    "- what's next? what conclusion can be drawn? what comes next for analyzing the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium documentation (for reader reference): \n",
    "* https://selenium-python.readthedocs.io/\n",
    "* https://www.selenium.dev/documentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing our libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#booting up selenium to automate the scraping of reddit\n",
    "\n",
    "#defining proxy ip\n",
    "proxy = \"20.235.159.154:80\"\n",
    "\n",
    "#defining headless mode (if we want to run this operation without gui)\n",
    "options= webdriver.FirefoxOptions()\n",
    "options.timeouts= { 'script': 5000 }\n",
    "options.add_argument(\"-headless\")\n",
    "options.add_argument(f\"--proxy-server={proxy}\")\n",
    "browser = webdriver.Firefox(options=options)\n",
    "\n",
    "#defining non-headless mode\n",
    "#browser= webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to help us get the number of instances of words in each post\n",
    "def countwords(textblock, keyword):\n",
    "    textblock= textblock.lower()\n",
    "    \n",
    "    return textblock.count(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to open a link to a page\n",
    "def openlink(url):\n",
    "    browser.get(url) #open the url in browser (we will be using firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to get title and body text of post\n",
    "#also get info like time, link, etc which may be helpful\n",
    "\n",
    "def getpostinfo(url):\n",
    "    #opening the link\n",
    "    openlink(url)\n",
    "    data= []#defining an empty list for post data we want to return\n",
    "    \n",
    "    #select all the posts on the page and get its contents in the form of a list of elements\n",
    "    posts= browser.find_elements(By.CSS_SELECTOR, \"div[class='rounded-3xl bg-surface-100-800-token max-w-5xl w-full p-4 variant-ghost-surface my-3']\")\n",
    "\n",
    "    #scraping results in a daily query\n",
    "    for post in posts:\n",
    "        postinfo= {} #empty dict for information on every post\n",
    "        title= post.find_element(By.TAG_NAME, 'h1').text #get title\n",
    "        #count up keywords instances in title\n",
    "        unalivecount= countwords(title, 'unalive')\n",
    "        killcount= countwords(title, 'kill')\n",
    "        suicidecount= countwords(title, 'suicide')\n",
    "        \n",
    "        #selecting the body text of the post\n",
    "        fullbody= post.find_element(By.CSS_SELECTOR, \"div[class='mt-2 overflow-hidden']\")\n",
    "        lines= fullbody.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "        bodytext=\"\"\n",
    "        for line in lines:\n",
    "            bodytext+= (\"\\n\"+line.text)\n",
    "        \n",
    "        #counting instances of the keywords in post body and adding it to total\n",
    "        unalivecount+= countwords(bodytext, 'unalive')\n",
    "        killcount+= countwords(bodytext, 'kill')\n",
    "        suicidecount+= countwords(suicidecount, 'kill')\n",
    "            \n",
    "        footnote= post.find_elements(By.CSS_SELECTOR, \"p[class='text-xs font-semibold'\")\n",
    "        \n",
    "        postinfo['time']= footnote[3].text #TODO: WRITE HELPER FUNCTION TO CONVERT DATE TO USEFUL FORMAT\n",
    "        postinfo['unalivecount']= unalivecount\n",
    "        postinfo['killcount']= killcount\n",
    "        postinfo['suicidecount']= suicidecount\n",
    "        postinfo['title']= title\n",
    "        postinfo['text']= bodytext\n",
    "        #postinfo['link']= footnote[4].text\n",
    "        \n",
    "        data.append(postinfo)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit caps search results at 100, so instead we will separate our query by day and scrape the results\n",
    "\n",
    "#defining the day we want to start querying. we will query every day from 1/1/2019 to 1/1/2025\n",
    "firstdayend= datetime.datetime(2019, 1, 1, 0, 0)\n",
    "before= time.mktime(firstdayend.timetuple())\n",
    "firstdaystart= datetime.datetime(2018, 12, 31, 23, 59)\n",
    "after= time.mktime(firstdaystart.timetuple())\n",
    "\n",
    "#defining the end of our sampling to be 1/1/2025\n",
    "lastdayend= datetime.datetime(2025, 1, 1, 0, 0)\n",
    "end= time.mktime(lastdayend.timetuple())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetfirstend():\n",
    "    return time.mktime(firstdayend.timetuple())\n",
    "\n",
    "def resetfirststart():\n",
    "    return time.mktime(firstdaystart.timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping for 'unalive'\n",
    "postdata_unalive= [] #defining empty list for the data\n",
    "while before<end:\n",
    "    url= f\"https://search-new.pullpush.io/?subreddit=offmychest&type=submission&q=unalive&sort_type=created_utc&sort=asc&before={before}&after={after}\"\n",
    "    scrapedata= getpostinfo(url)\n",
    "    postdata_unalive.append({'keyword' : \"unalive\", \n",
    "                     'time' : datetime.datetime.fromtimestamp(int(before)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                     'numposts' : len(scrapedata),\n",
    "                     'data' : scrapedata})\n",
    "    after=before\n",
    "    before=before+86400 #each day is incremented by 86400 seconds\n",
    "\n",
    "#reset the start parameters  \n",
    "before= resetfirstend()\n",
    "after= resetfirststart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping for 'kill'\n",
    "postdata_kill= [] #defining empty list for the data\n",
    "while before<end:\n",
    "    url= f\"https://search-new.pullpush.io/?subreddit=offmychest&type=submission&q=kill&sort_type=created_utc&sort=asc&before={before}&after={after}\"\n",
    "    scrapedata= getpostinfo(url)\n",
    "    postdata_kill.append({'keyword' : \"kill\", \n",
    "                     'time' : datetime.datetime.fromtimestamp(int(before)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                     'numposts' : len(scrapedata),\n",
    "                     'data' : scrapedata})\n",
    "    after=before\n",
    "    before=before+86400 #each day is incremented by 86400 seconds\n",
    "    \n",
    "#reset the start parameters  \n",
    "before= resetfirstend()\n",
    "after= resetfirststart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping for 'suicide'\n",
    "postdata_suicide= [] #defining empty list for the data\n",
    "while before<end:\n",
    "    url= f\"https://search-new.pullpush.io/?subreddit=offmychest&type=submission&q=suicide&sort_type=created_utc&sort=asc&before={before}&after={after}\"\n",
    "    scrapedata= getpostinfo(url)\n",
    "    postdata_suicide.append({'keyword' : \"suicide\", \n",
    "                     'time' : datetime.datetime.fromtimestamp(int(before)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                     'numposts' : len(scrapedata),\n",
    "                     'data' : scrapedata})\n",
    "    after=before\n",
    "    before=before+86400 #each day is incremented by 86400 seconds\n",
    "    \n",
    "#reset the start parameters  \n",
    "before= resetfirstend()\n",
    "after= resetfirststart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine our data into a singular list\n",
    "postdata= postdata_unalive+postdata_kill+postdata_suicide\n",
    "\n",
    "#after collecting data, export it into csv format\n",
    "header = [\"keyword\", \"time\", \"numposts\", \"data\"]\n",
    "with open('data.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    for row in postdata:\n",
    "        writer.writerow(row)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
