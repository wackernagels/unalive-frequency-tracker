{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final project items\n",
    "- descibe the data and a summary of the stats\n",
    "- state the research question, and what exactly we're trying to express with the data\n",
    "- what is the method? why this visual? why is this one chosen?\n",
    "- what's next? what conclusion can be drawn? what comes next for analyzing the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium documentation (for reader reference): \n",
    "* https://selenium-python.readthedocs.io/\n",
    "* https://www.selenium.dev/documentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing our libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#booting up selenium to automate the scraping of reddit\n",
    "\n",
    "#defining headless mode (if we want to run this operation without gui)\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.add_argument(\"-headless\")\n",
    "browser = webdriver.Firefox(options=options)\n",
    "\n",
    "#defining non-headless mode\n",
    "#browser= webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to help us get the number of instances of words in each post\n",
    "def countwords(textblock, keyword):\n",
    "    textblock= textblock.lower()\n",
    "    \n",
    "    return textblock.count(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to open a link to a page\n",
    "\n",
    "def openlink(url):\n",
    "    browser.get(url) #open the url in browser (we will be using firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to get title and body text of post\n",
    "#also get info like time, link, etc which may be helpful\n",
    "\n",
    "def getpostinfo(url):\n",
    "    #opening the link\n",
    "    openlink(url)\n",
    "    time.sleep(3) #tell code to pause for a while so the page can load\n",
    "    data= []#defining an empty list for post data we want to return\n",
    "    \n",
    "    #select all the posts on the page and get its contents in the form of a list of elements\n",
    "    posts= browser.find_elements(By.CSS_SELECTOR, \"div[class='rounded-3xl bg-surface-100-800-token max-w-5xl w-full p-4 variant-ghost-surface my-3']\")\n",
    "\n",
    "    #scraping results in a daily query\n",
    "    for post in posts:\n",
    "        print(\"iterating posts\")\n",
    "        postinfo= {} #empty dict for information on every post\n",
    "        title= post.find_element(By.TAG_NAME, 'h1').text #get title\n",
    "        #count up keywords instances in title\n",
    "        unalivecount= countwords(title, 'unalive')\n",
    "        killcount= countwords(title, 'kill')\n",
    "        \n",
    "        #selecting the body text of the post\n",
    "        fullbody= post.find_element(By.CSS_SELECTOR, \"div[class='mt-2 overflow-hidden']\")\n",
    "        lines= fullbody.find_elements(By.CSS_SELECTOR, \"p\")\n",
    "        bodytext=\"\"\n",
    "        for line in lines:\n",
    "            bodytext+= (\"\\n\"+line.text)\n",
    "        \n",
    "        #counting instances of the keywords in post body and adding it to total\n",
    "        unalivecount+= countwords(bodytext, 'unalive')\n",
    "        killcount+= countwords(bodytext, 'kill')\n",
    "            \n",
    "        footnote= post.find_elements(By.CSS_SELECTOR, \"p[class='text-xs font-semibold'\")\n",
    "        \n",
    "        postinfo['time']= footnote[3].text #TODO: WRITE HELPER FUNCTION TO CONVERT DATE TO USEFUL FORMAT\n",
    "        postinfo['unalivecount']= unalivecount\n",
    "        postinfo['killcount']= killcount\n",
    "        postinfo['title']= title\n",
    "        postinfo['text']= bodytext\n",
    "        #postinfo['link']= footnote[4].text\n",
    "        \n",
    "        data.append(postinfo)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n",
      "iterating posts\n"
     ]
    }
   ],
   "source": [
    "#reddit caps search results at 100, so instead we will separate our query by day and scrape the results\n",
    "\n",
    "#defining the day we want to start querying. we will query every day from 1/1/2015 to 1/31/2025\n",
    "#january 1 2015 (1/1/2015) at 11:59 pm\n",
    "before= 1419984000\n",
    "#december 31 2014 (12/31/2014) at 12:00 am\n",
    "after= 1419897600\n",
    "#each day is incremented by 86400 seconds\n",
    "\n",
    "#TO DO: USE PULL PUSH IO INSTEAD OF REDDIT\n",
    "#SCRAPE FOR EACH DAY AND LOG DATE AND CONTENT OF THE POST\n",
    "\n",
    "#list of scraped post data, which will be populated with the text that gets scraped\n",
    "postdata= {}\n",
    "\n",
    "url= \"https://search-new.pullpush.io/?subreddit=offmychest&type=submission&q=kill&sort_type=created_utc&sort=desc&before=1419897600&after=1419811200\"\n",
    "scrapedata= getpostinfo(url)\n",
    "postdata['datetime']= {'numposts' : len(scrapedata), 'data' : scrapedata}\n",
    "    \n",
    "#print(getpostinfo(url, keyword))\n",
    "\n",
    "'''\n",
    "{'unalive' : [], 'kill' : [], 'suicide' : [], 'murder' : []}\n",
    "while before<1738389600:\n",
    "    url= f\"https://search-new.pullpush.io/?subreddit=offmychest&type=submission&q=unalive&sort_type=created_utc&sort=asc&before={before}&after={after}\"\n",
    "    scrapedata= getpostinfo(url)\n",
    "    postdata['unalive'].append({'time' : before, 'numposts' : len(scrapedata), 'data' : scrapedata})\n",
    "    \n",
    "    url= f\"https://search-new.pullpush.io/?subreddit=offmychest&type=submission&q=kill&sort_type=created_utc&sort=asc&before={before}&after={after}\"\n",
    "    scrapedata= getpostinfo(url)\n",
    "    postdata['kill'].append('time' : before, 'numposts' : len(scrapedata), 'data' : scrapedata})\n",
    "    \n",
    "    url= f\"https://search-new.pullpush.io/?subreddit=offmychest&type=submission&q=suicide&sort_type=created_utc&sort=asc&before={before}&after={after}\"\n",
    "    scrapedata= getpostinfo(url)\n",
    "    postdata['suicide'].append('time' : before, 'numposts' : len(scrapedata), 'data' : scrapedata})\n",
    "    \n",
    "    url= f\"https://search-new.pullpush.io/?subreddit=offmychest&type=submission&q=murder&sort_type=created_utc&sort=asc&before={before}&after={after}\"\n",
    "    scrapedata= getpostinfo(url)\n",
    "    postdata['murder'].append('time' : before, 'numposts' : len(scrapedata), 'data' : scrapedata})\n",
    "    \n",
    "    after=before\n",
    "    before=before+86400\n",
    "\n",
    "'''\n",
    "\n",
    "#after collecting data, export it into csv format\n",
    "with open('data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(postdata.keys())\n",
    "    rows = zip(postdata.values())\n",
    "    writer.writerows(rows)\n",
    "    \n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
